---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

```{r}
library(jsonlite)
library(tm)
library(qdap)
library(overlap)
library(methods)
library(quanteda)
library(dplyr)
library(tidytext)
library(stringr)
library(officer)
library(glmnet)
library(knitr)
library(stringi)


job_desc <- jsonlite::fromJSON('~/Downloads/indeed_job_descs_2021_03_16.json')
names(job_desc)
job_desc$request_params[[1]] #not na 1
job_desc$job_descriptions[[45]]  # not na 2
len <- function(x) {
  return(length(x))
}

l <- sapply(job_desc$job_descriptions,length)
l_p <- sapply(job_desc$request_params,length)
description <- job_desc$job_descriptions

desc = c()
for (i in 1:length(description)){
  for (j in 1:length(description[[i]])){
    if (is.na(description[[i]][j])){
      next
    }
    desc <- append(desc,description[[i]][j])
  }
}


# data("data_corpus_inaugural", package = "quanteda")
d <- quanteda::dfm(desc, verbose = FALSE)

dim(d)

#l <- sapply(desc,len)
#desc_dtm <- DocumentTermMatrix(desc[[1]])
target_freq <- as.numeric(d)
freqs_mat <- as.matrix(d)
doc_freq <- apply(freqs_mat,2,function(x) mean(x>0))
idf <- 1/doc_freq
idf_mat <- rep(idf,nrow(freqs_mat), byrow = TRUE, nrow = nrow(freqs_mat))
tf_idf <- freqs_mat * idf_mat

```


```{r pca}
pr_out <- prcomp(tf_idf, scale=TRUE) #look at names of pr_out 
eigen_val <- pr_out$sdev^2 
plot(cumsum(eigen_val) / sum(eigen_val))
abline(h=.9)
plot(pr_out$sdev)

# data mining approach 
k <- 3 # could be anywhere from 2-4
plot(pr_out$rotation[, k])
head(pr_out$rotation[,k]) #these are the loadings 
abline(h = 0)
which(abs(pr_out$rotation[, k]) > 0.08)
```

The some of the loadings are who, we, are, :, cardinal, and financial. 
NOTE: I am wondering if our word-separation method is the best we could use because the loadings include a lot of things like punctuation marks. 

There are a lot of words with high PCA values. 
NOTE: should we keep the websites in? 
TF-IDF does seem to be important though because it has revealed that the job descriptions are asking for very specific things like "agtech" or "adventurous" very frequently.  


```{r remove stopwords and punctuations}
desc_cleaned <- c()
for (i in seq_along(desc)){
    without_stopwords <- rm_stopwords(
    desc[i],
    stopwords = qdapDictionaries::Top200Words, 
    # ADD c("work experience", "skills", "employment", "objective", "awards", "honors", "gpa")
    unlist = FALSE,
    separate = TRUE,
    strip = FALSE,
    unique = FALSE,
    char.keep = NULL,
    names = FALSE,
    ignore.case = TRUE,
    apostrophe.remove = FALSE
  )
  combine_1 <- combine_words(
    without_stopwords[[1]],
    sep = " "
  )
  combine =paste("", combine_1,"")
  desc_cleaned <- append(desc_cleaned, removePunctuation(combine))
}

# the dim of d is 590*16202. After removing all the punctuations and stopwords, its dimension is 590*13732.
d_cleaned <- quanteda::dfm(desc_cleaned, verbose = FALSE)

dim(d_cleaned)

#l <- sapply(desc,len)
#desc_dtm <- DocumentTermMatrix(desc[[1]])
target_freq_1 <- as.numeric(d_cleaned)
freqs_mat_1 <- as.matrix(d_cleaned)
doc_freq_1 <- apply(freqs_mat_1,2,function(x) mean(x>0))
idf_1 <- 1/doc_freq_1
idf_mat_1 <- rep(idf_1,nrow(freqs_mat_1), byrow = TRUE, nrow = nrow(freqs_mat_1))
tf_idf_1 <- freqs_mat_1 * idf_mat_1

```

```{r}
# get the top k tokens with the highest tf-idf value
k <- 15
i <- 1
keyword_lists <- data.frame(matrix(NA, nrow = nrow(tf_idf_1), ncol = k))
for (i in 1:nrow(tf_idf_1)){
  keyword_lists[i,] <- names(tf_idf_1[i,][order(tf_idf_1[i,],decreasing = TRUE)[1:k]])
}

```
For each job description(each row in the tf_ids_1), I get the top `r k` tokens with the highest tf-idf value. We can also extract the top 5 tokens with highest tf-idf scores in our resume, and see if there will be some tokens overlapping between the job description and the resume.

```{r trying to extract keyword in resume using tf_idf}
# give priority on the resume. 
# cooccurance- build up association between words 
#data(stop_words) # Stop words.
real_resume <- read_docx("Li_Jiaxin_resume.docx")
real_resume <- str_replace_all(real_resume, pattern = '\"', replacement = "")

# real_resume <- paste(real_resume,collapse="")
# transform it into a term document matrix
resume <- quanteda::dfm(real_resume, verbose = FALSE)

target_freq_resume <- as.numeric(resume)
freqs_mat_resume <- as.matrix(resume)
doc_freq_resume <- apply(freqs_mat_resume,2,function(x) mean(x>0))
idf_resume <- 1/doc_freq_resume
idf_mat_resume <- rep(idf_resume,nrow(freqs_mat_resume), byrow = TRUE, nrow = nrow(freqs_mat_resume))
tf_idf_resume <- freqs_mat_resume * idf_mat_resume

keywords_in_resume <- names(tf_idf_resume[1,][order(tf_idf_1[1,],decreasing = TRUE)[1:k]])
```

```{r}
overlap_keyword <- c()
for (i in 1:nrow(tf_idf_1)){
  keyword_job_desc <- key_words[i,]
  overlap_keyword[i] <- length(intersect(keywords_in_resume,keyword_job_desc))
}

```

It is awkward that none of the job desc has over lap with my resume...

```{r tring another way to extract most frequent tokens as keywords in my resume}
text <- paste(real_resume,collapse="")
text <- paste(text, collapse = " ")
text <- str_replace_all(text, pattern = '\"', replacement = "") # Remove slashes
text <- str_replace_all(text, pattern = '\n', replacement = "") # Remove \n
text <- str_replace_all(text, pattern = '\u0092', replacement = "'") #Replace with quote
text <- str_replace_all(text, pattern = '\u0091', replacement = "'") #Replace with quote


text_df <- data_frame(Text = text) # tibble aka neater data frame

text_words <- text_df %>% unnest_tokens(output = word, input = Text) 

text_words  <- text_words  %>% anti_join(stop_words) 

text_wordcounts <- text_words  %>% count(word, sort = TRUE)
keywords_in_resume <- text_wordcounts$word[1:100]

overlap_keyword <- c()
for (i in 1:nrow(tf_idf_1)){
  keyword_job_desc <- key_words[i,]
  overlap_keyword[i] <- length(intersect(keywords_in_resume,keyword_job_desc))
}
sum(overlap_keyword)
```
Still doesn't work...


```{r KATE RESUME: trying to extract keyword in resume using tf_idf}

# give priority on the resume. 
# cooccurance- build up association between words 
data(stop_words) # Stop words.
real_resume_k <- readLines("~/Downloads/MarshResumeJan2021_Data_Mining.txt")
real_resume_k <- str_replace_all(real_resume, pattern = '\"', replacement = "")

# real_resume <- paste(real_resume,collapse="")
# transform it into a term document matrix
resume <- quanteda::dfm(real_resume_k, verbose = FALSE)
#non_stop_cols <- stopwords(colnames(resume))
stops <- stopwords("smart")
cols <- removePunctuation(colnames(resume), preserve_intra_word_contractions = TRUE, preserve_intra_word_dashes = TRUE)
#colnames(cols["klm2244columbiaedu"]) <- "klm2244columbia.edu"
keep_cols <- cols[!cols %in% stops]# colnames w/o stopwords
keep_cols <- rm_number(cols, trim = TRUE, clean = TRUE)
keep_cols <- stri_remove_empty(keep_cols)
resume <- resume[, keep_cols]


target_freq_resume <- as.numeric(resume)
freqs_mat_resume <- as.matrix(resume)
doc_freq_resume <- apply(freqs_mat_resume,2,function(x) mean(x>0))
idf_resume <- 1/doc_freq_resume
idf_mat_resume <- rep(idf_resume,nrow(freqs_mat_resume), byrow = TRUE, nrow = nrow(freqs_mat_resume))
tf_idf_resume <- freqs_mat_resume * idf_mat_resume

keywords_in_resume <- names(tf_idf_resume[1,][order(tf_idf_1[1,],decreasing = TRUE)[1:k]])
#keywords_in_resume <- str_replace_all(keywords_in_resume, pattern = '-', replacement = "") #Remove dashes

```

```{r KATE RESUME}
overlap_keyword <- c()
for (i in 1:nrow(tf_idf_1)){
  keyword_job_desc <- keyword_lists[i,]
  overlap_keyword[i] <- length(intersect(keywords_in_resume,keyword_job_desc))
}
#head(overlap_keyword)

```
Overlap!!

```{r KATE RESUME: trying another way to extract most frequent tokens as keywords in my resume}
text <- paste(real_resume_k,collapse="")
text <- paste(text, collapse = " ")
text <- str_replace_all(text, pattern = '\"', replacement = "") # Remove slashes
text <- str_replace_all(text, pattern = '\n', replacement = "") # Remove \n
text <- str_replace_all(text, pattern = '-', replacement = "") #Remove dashes
text <- str_replace_all(text, pattern = '[0-9]+', replacement = "") #Remove dashes
text <- str_replace_all(text, pattern = '\u0092', replacement = "'") #Replace with quote
text <- str_replace_all(text, pattern = '\u0091', replacement = "'") #Replace with quote

text_df <- data_frame(Text = text) # tibble aka neater data frame
text_words <- text_df %>% unnest_tokens(output = word, input = Text) 
text_words  <- text_words  %>% anti_join(stop_words) 

text_wordcounts <- text_words  %>% count(word, sort = TRUE)
keywords_in_resume <- text_wordcounts$word[1:100]

overlap_keyword <- c()
#overlaps <- c()
for (i in 1:nrow(tf_idf_1)){
  keyword_job_desc <- keyword_lists[i,]
  overlap_keyword[i] <- length(intersect(keywords_in_resume,keyword_job_desc))
  #overlaps[i] <- intersect(keywords_in_resume,keyword_job_desc)
  # rbind(overlaps[i], fill = TRUE) this 
}
sum(overlap_keyword)


highly_relevant <- length(which(overlap_keyword>=4))
head(tf_idf_1[highly_relevant,]) 
head(desc[highly_relevant])
overlap_keyword[26]
desc[26]

intersect(which(grepl("sustainab", desc, ignore.case = TRUE)), highly_relevant)

desc[] #checking on things that would be close to my resume 
sort(tf_idf_1[847,],decreasing=TRUE) 
  
#rownames(tf_idf_1) == tf_idf_1[overlap_keyword,]

## Overlap keywords matrix 

```

```{r}

# bind tf_idf_1 
#log_df1 = cbind(d, as.matrix(tf_idf_1))
#log_df <- log_df1[,unique(colnames(log_df1))]
mat_tf_idf <- as.matrix(tf_idf_1)

#hclus_out <- hclust(dist(cbind(x, y)), "single") #cbind to make a matrix 
hclus_out <- hclust(dist(log_df1), "complete") #cbind to make a matrix 
plot(hclus_out)
clus_est <- cutree(hclus_out, k=8)
plot(tf_idf_1, pch=16,
     col=c("red", "blue", "black", "green", "purple", "orange", "yellow", "grey", "pink", "navy","tan", "violet", "cyan")[clus_est])

clus_est

desc[,c(263, 323)]

```


```{r}
km_out <- kmeans(mat_tf_idf, centers = 8)
names(km_out) # $cluster is the assignment to the cluster 
length(km_out$cluster)
table(km_out$cluster)
# km_out$centers 
km_out$centers 
# j is the TRUE assignment 
mean(km_out$cluster==j) # xbar and ybar in the different groups 
# NOT a good way to evaluate 
table(km_out$cluster, j)


cols <- c("red", "blue", "green", "purple", "yellow", "orange", "pink", "grey")
plot(tf_idf_1,
     col=cols[km_out$cluster],
     pch=16)
```

```{r}
# TF-IDF and cosine similarity
#tfidf <- t(dtm[ , tf_mat$term ]) * tf_mat$idf
#tfidf <- t(tfidf)

# changing cosine similarity to a distance 
csim <- tf_idf_1 / sqrt(rowSums(tf_idf_1 * tf_idf_1))
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim)

#h clust 
k <- 18
hc <- hclust(cdist, "complete")
clustering <- cutree(hc, k)
plot(hc, main = "Hierarchical Clustering of TF-IDF")
rect.hclust(hc, k, border = "red")

p_words <- colSums(freqs_mat_1) / sum(freqs_mat_1)
cluster_words <- lapply(unique(clustering), function(x){
  rows <- freqs_mat_1[clustering == x,]
  # for memory's sake, drop all words that don't appear in the cluster
  rows <- rows[ , colSums(rows) > 0 ]
  colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
```

```{r visualization}
cluster_summary <- data.frame(cluster = unique(clustering),
                              size = as.numeric(table(clustering)),
                              top_words = sapply(cluster_words, function(d){
                                paste(
                                  names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                  collapse = ", ")
                              }),
                              stringsAsFactors = FALSE)
cluster_summary

library(wordcloud)
wordcloud::wordcloud(words = names(cluster_words[[ 4 ]]), 
                     freq = cluster_words[[ 4 ]], 
                     max.words = 25, 
                     random.order = FALSE, 
                     colors = c("red", "yellow", "blue1", 
                                "green1", "purple", "black", 
                                "grey", "orange", "pink", 
                                "blue2", "green2", "purple2",
                                "yellow2", "blue3", "green3",
                                "pink2", "red2", "orange2"
                                ),
                     main = "Top words in cluster 100")

```

```{r logistic model}

log_df1 = cbind(d, as.matrix(tf_idf))
log_df <- log_df1[,unique(colnames(log_df1))]
#log_df<-sapply(log_df1, function(x) as.character(x))

# this does not work. I am not sure why. 

is_train <- sample(nrow(log_df),nrow(log_df)*0.80)
job.train = log_df[is_train,]
job.test = log_df[-is_train,]

logit_mod <- glm(docs ~ .,
                    family = "binomial",
                    data = job.train)
pred.glm = as.numeric(predict(logit_mod, job.test, type="response") > 0.5)
```


