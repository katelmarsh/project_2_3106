---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

```{r}
library(jsonlite)
library(tm)
library(qdap)
library(overlap)
library(methods)
library(quanteda)
library(dplyr)
library(tidytext)
library(knitr)
library(qdap)

job_desc <- jsonlite::fromJSON('~/Downloads/indeed_job_descs_2020_09_20.json')
names(job_desc)
job_desc$request_params[[1]] #not na 1
job_desc$job_descriptions[[45]]  # not na 2
len <- function(x) {
  return(length(x))
}

l <- sapply(job_desc$job_descriptions,length)
l_p <- sapply(job_desc$request_params,length)
description <- job_desc$job_descriptions

desc = c()
for (i in 1:length(description)){
  for (j in 1:length(description[[i]])){
    if (is.na(description[[i]][j])){
      next
    }
    desc <- append(desc,description[[i]][j])
  }
}


# data("data_corpus_inaugural", package = "quanteda")
d <- quanteda::dfm(desc, verbose = FALSE)

dim(d)

#l <- sapply(desc,len)
#desc_dtm <- DocumentTermMatrix(desc[[1]])
target_freq <- as.numeric(d)
freqs_mat <- as.matrix(d)
doc_freq <- apply(freqs_mat,2,function(x) mean(x>0))
idf <- 1/doc_freq
idf_mat <- rep(idf,nrow(freqs_mat), byrow = TRUE, nrow = nrow(freqs_mat))
tf_idf <- freqs_mat * idf_mat

```


```{r json in a different way}
library(jsonlite)
library(tm)
library(qdap)
library(overlap)
library(methods)
library(quanteda)
library(dplyr)
library(tidytext)
library(knitr)
library(qdap)

job_desc <- read_json('~/Downloads/indeed_job_descs_2020_09_20.json')

job_descriptions_list <- c()
for(j in seq_len(length(job_desc))){
  for (ii in seq_len(length(job_desc[[j]]$job_descriptions))){
    job_descriptions_list[ii] <- job_desc[[j]]$job_descriptions[[ii]]
  }
}
head(job_descriptions_list)

d <- quanteda::dfm(job_descriptions_list, verbose = FALSE)

dim(d)

#l <- sapply(desc,len)
#desc_dtm <- DocumentTermMatrix(desc[[1]])
target_freq <- as.numeric(d)
freqs_mat <- as.matrix(d)
doc_freq <- apply(freqs_mat,2,function(x) mean(x>0))
idf <- 1/doc_freq
idf_mat <- rep(idf,nrow(freqs_mat), byrow = TRUE, nrow = nrow(freqs_mat))
tf_idf <- freqs_mat * idf_mat



split_job_description <- function(job_descriptions_entry){
  transform_dat <- gsub("[^[:alnum:]_]", " ", job_descriptions_entry)
  # an alternative solution would be gsub("[^a-zA-Z0-9_]", " ", job_descriptions_entry)
  transform_dat <- strsplit(transform_dat, split=" ")[[1]]
  transform_dat <- transform_dat[transform_dat!=""]
  return(transform_dat)
}

job_descriptions_list <- lapply(job_desc$job_descriptions, split_job_description)

```

```{r pca}
pr_out <- prcomp(tf_idf, scale=TRUE) #look at names of pr_out 
eigen_val <- pr_out$sdev^2 
plot(cumsum(eigen_val) / sum(eigen_val))
abline(h=.9)
plot(pr_out$sdev)

# data mining approach 
k <- 3 # could be anywhere from 2-4
plot(pr_out$rotation[, k])
head(pr_out$rotation[,k]) #these are the loadings 
abline(h = 0)
which(abs(pr_out$rotation[, k]) > 0.05)
```

The some of the loadings are who, we, are, :, cardinal, and financial. 
NOTE: I am wondering if our word-separation method is the best we could use because the loadings include a lot of things like punctuation marks. 

There are a lot of words with high PCA values. 
NOTE: should we keep the websites in? 
TF-IDF does seem to be important though because it has revealed that the job descriptions are asking for very specific things like "agtech" or "adventurous" very frequently.  


```{r remove stopwords and punctuations}
desc_cleaned <- c()
for (i in seq_along(desc)){
    without_stopwords <- rm_stopwords(
    desc[i],
    stopwords = qdapDictionaries::Top200Words,
    unlist = FALSE,
    separate = TRUE,
    strip = FALSE,
    unique = FALSE,
    char.keep = NULL,
    names = FALSE,
    ignore.case = TRUE,
    apostrophe.remove = FALSE
  )
  combine_1 <- combine_words(
    without_stopwords[[1]],
    sep = " "
  )
  combine =paste("", combine_1,"")
  desc_cleaned <- append(desc_cleaned, removePunctuation(combine))
}

# the dim of d is 590*16202. After removing all the punctuations and stopwords, its dimension is 590*13732.
d_cleaned <- quanteda::dfm(desc_cleaned, verbose = FALSE)

dim(d_cleaned)

#l <- sapply(desc,len)
#desc_dtm <- DocumentTermMatrix(desc[[1]])
target_freq_1 <- as.numeric(d_cleaned)
freqs_mat_1 <- as.matrix(d_cleaned)
doc_freq_1 <- apply(freqs_mat_1,2,function(x) mean(x>0))
idf_1 <- 1/doc_freq_1
idf_mat_1 <- rep(idf_1,nrow(freqs_mat_1), byrow = TRUE, nrow = nrow(freqs_mat_1))
tf_idf_1 <- freqs_mat_1 * idf_mat_1

```

```{r}
# get the top k tokens with the highest tf-idf value
k <- 10
i <- 1
keyword_lists <- data.frame(matrix(NA, nrow = nrow(tf_idf_1), ncol = k))
for (i in 1:nrow(tf_idf_1)){
  keyword_lists[i,] <- names(tf_idf_1[i,][order(tf_idf_1[i,],decreasing = TRUE)[1:k]])
}

```
For each job description(each row in the tf_ids_1), I get the top `r k` tokens with the highest tf-idf value. We can also extract the top 5 tokens with highest tf-idf scores in our resume, and see if there will be some tokens overlapping between the job description and the resume.

```{r trying to extract keyword in resume using tf_idf}
# give priority on the resume. 
# cooccurance- build up association between words 
data(stop_words) # Stop words.
real_resume <- read_docx("Li_Jiaxin_resume.docx")
real_resume <- str_replace_all(real_resume, pattern = '\"', replacement = "")


# real_resume <- paste(real_resume,collapse="")
# transform it into a term document matrix
resume <- quanteda::dfm(real_resume, verbose = FALSE)

target_freq_resume <- as.numeric(resume)
freqs_mat_resume <- as.matrix(resume)
doc_freq_resume <- apply(freqs_mat_resume,2,function(x) mean(x>0))
idf_resume <- 1/doc_freq_resume
idf_mat_resume <- rep(idf_resume,nrow(freqs_mat_resume), byrow = TRUE, nrow = nrow(freqs_mat_resume))
tf_idf_resume <- freqs_mat_resume * idf_mat_resume

keywords_in_resume <- names(tf_idf_resume[1,][order(tf_idf_1[1,],decreasing = TRUE)[1:k]])
```

```{r}
overlap_keyword <- c()
for (i in 1:nrow(tf_idf_1)){
  keyword_job_desc <- key_words[i,]
  overlap_keyword[i] <- length(intersect(keywords_in_resume,keyword_job_desc))
}

```

It is awkward that none of the job desc has over lap with my resume...

```{r tring another way to extract most frequent tokens as keywords in my resume}
text <- paste(real_resume,collapse="")
text <- paste(text, collapse = " ")
text <- str_replace_all(text, pattern = '\"', replacement = "") # Remove slashes
text <- str_replace_all(text, pattern = '\n', replacement = "") # Remove \n
text <- str_replace_all(text, pattern = '\u0092', replacement = "'") #Replace with quote
text <- str_replace_all(text, pattern = '\u0091', replacement = "'") #Replace with quote


text_df <- data_frame(Text = text) # tibble aka neater data frame

text_words <- text_df %>% unnest_tokens(output = word, input = Text) 


text_words  <- text_words  %>% anti_join(stop_words) 


text_wordcounts <- text_words  %>% count(word, sort = TRUE)
keywords_in_resume <- text_wordcounts$word[1:100]

overlap_keyword <- c()
for (i in 1:nrow(tf_idf_1)){
  keyword_job_desc <- key_words[i,]
  overlap_keyword[i] <- length(intersect(keywords_in_resume,keyword_job_desc))
}
sum(overlap_keyword)
```
Still doesn't work...


```{r KATE RESUME: trying to extract keyword in resume using tf_idf}
library(stringr)
library(officer)
# give priority on the resume. 
# cooccurance- build up association between words 
data(stop_words) # Stop words.
real_resume <- read_docx("MarshResumeJan2021_for_class.docx")
real_resume <- str_replace_all(real_resume, pattern = '\"', replacement = "")


# real_resume <- paste(real_resume,collapse="")
# transform it into a term document matrix
resume <- quanteda::dfm(real_resume, verbose = FALSE)

target_freq_resume <- as.numeric(resume)
freqs_mat_resume <- as.matrix(resume)
doc_freq_resume <- apply(freqs_mat_resume,2,function(x) mean(x>0))
idf_resume <- 1/doc_freq_resume
idf_mat_resume <- rep(idf_resume,nrow(freqs_mat_resume), byrow = TRUE, nrow = nrow(freqs_mat_resume))
tf_idf_resume <- freqs_mat_resume * idf_mat_resume

keywords_in_resume <- names(tf_idf_resume[1,][order(tf_idf_1[1,],decreasing = TRUE)[1:k]])
```

```{r KATE RESUME}
overlap_keyword <- c()
for (i in 1:nrow(tf_idf_1)){
  keyword_job_desc <- key_words[i,]
  overlap_keyword[i] <- length(intersect(keywords_in_resume,keyword_job_desc))
}

```

It is awkward that none of the job desc has over lap with my resume...

```{r KATE RESUME: trying another way to extract most frequent tokens as keywords in my resume}
text <- paste(real_resume,collapse="")
text <- paste(text, collapse = " ")
text <- str_replace_all(text, pattern = '\"', replacement = "") # Remove slashes
text <- str_replace_all(text, pattern = '\n', replacement = "") # Remove \n
text <- str_replace_all(text, pattern = '\u0092', replacement = "'") #Replace with quote
text <- str_replace_all(text, pattern = '\u0091', replacement = "'") #Replace with quote


text_df <- data_frame(Text = text) # tibble aka neater data frame

text_words <- text_df %>% unnest_tokens(output = word, input = Text) 


text_words  <- text_words  %>% anti_join(stop_words) 


text_wordcounts <- text_words  %>% count(word, sort = TRUE)
keywords_in_resume <- text_wordcounts$word[1:100]

overlap_keyword <- c()
for (i in 1:nrow(tf_idf_1)){
  keyword_job_desc <- key_words[i,]
  overlap_keyword[i] <- length(intersect(keywords_in_resume,keyword_job_desc))
}
sum(overlap_keyword)

```


```{r logistic model}
library(glmnet)

log_df1 = cbind(d, as.matrix(tf_idf))
log_df <- log_df1[,unique(colnames(log_df1))]
#log_df<-sapply(log_df1, function(x) as.character(x))

# this does not work. I am not sure why. 

is_train <- sample(nrow(log_df),nrow(log_df)*0.80)
job.train = log_df[is_train,]
job.test = log_df[-is_train,]

logit_mod <- glm(docs ~ .,
                    family = "binomial",
                    data = job.train)
pred.glm = as.numeric(predict(logit_mod, job.test, type="response") > 0.5)

```


